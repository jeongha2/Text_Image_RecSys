{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b0ff66",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mxnet\n",
    "!pip install gluonnlp tqdm\n",
    "!pip install sentencepiece\n",
    "!pip install transformers\n",
    "!pip install soynlp\n",
    "!pip install emoji\n",
    "!pip install AdamP\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data import TensorDataset, RandomSampler, SequentialSampler\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW, BertConfig\n",
    "from tqdm.notebook import tqdm\n",
    "from adamp import AdamP\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "#Dataset Preparing\n",
    "train_ds = pd.read_excel(\"./train.xlsx\")\n",
    "test_ds = pd.read_excel(\"./test.xlsx\")\n",
    "\n",
    "train_ds['Emotion'].value_counts()\n",
    "\n",
    "train_sentence, train_emotion = train_ds.Sentence, train_ds.Emotion\n",
    "test_sentence, test_emotion  = test_ds.Sentence, test_ds.Emotion\n",
    "train_sentence = [\"[CLS] \" + str(s) + \" [SEP]\" for s in train_sentence]\n",
    "test_sentence = [\"[CLS] \" + str(s) + \" [SEP]\" for s in test_sentence]\n",
    "\n",
    "#만약 KcELECTRA를 사용하기 원한다면 아래 코드를 이용하면 됩니다.\n",
    "#beomi/KcELECTRA-base\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"beomi/kcbert-base\", do_lower_case=False)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "Encoder = LabelEncoder()\n",
    "\n",
    "train_emotion =Encoder.fit_transform(train_emotion)\n",
    "test_emotion = Encoder.transform(test_emotion)\n",
    "\n",
    "x_train,x_valid,y_train,y_valid = train_test_split(train_sentence,train_emotion,test_size=0.3)\n",
    "\n",
    "y_train = y_train\n",
    "y_valid = y_valid\n",
    "y_test = test_emotion\n",
    "\n",
    "train_tokenized_texts = [tokenizer.tokenize(s) for s in x_train]\n",
    "valid_tokenized_texts = [tokenizer.tokenize(s) for s in x_valid]\n",
    "test_tokenized_texts = [tokenizer.tokenize(s) for s in test_sentence]\n",
    "\n",
    "MAX_LEN = 128 #최대 시퀀스 길이 설정\n",
    "train_input_ids = [tokenizer.convert_tokens_to_ids(x) for x in train_tokenized_texts]\n",
    "valid_input_ids = [tokenizer.convert_tokens_to_ids(x) for x in valid_tokenized_texts]\n",
    "test_input_ids = [tokenizer.convert_tokens_to_ids(x) for x in test_tokenized_texts]\n",
    "\n",
    "train_input_ids = pad_sequences(train_input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "valid_input_ids = pad_sequences(valid_input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "test_input_ids = pad_sequences(test_input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "\n",
    "def make_seg_mask(input_ids):\n",
    "  attention_mask = []\n",
    "  for seg in input_ids:\n",
    "    seg_mask = [float(i>0) for i in seg]\n",
    "    attention_mask.append(seg_mask)\n",
    "  \n",
    "  return attention_mask\n",
    "\n",
    "\n",
    "train_attention_masks = make_seg_mask(train_input_ids)\n",
    "valid_attention_masks = make_seg_mask(valid_input_ids)\n",
    "test_attention_masks = make_seg_mask(test_input_ids)\n",
    "\n",
    "\n",
    "train_inputs = torch.tensor(train_input_ids)\n",
    "train_labels = torch.tensor(y_train)\n",
    "train_masks = torch.tensor(train_attention_masks)\n",
    "\n",
    "validation_inputs = torch.tensor(valid_input_ids)\n",
    "validation_labels = torch.tensor(y_valid)\n",
    "validation_masks = torch.tensor(valid_attention_masks)\n",
    "\n",
    "test_inputs = torch.tensor(test_input_ids)\n",
    "test_labels = torch.tensor(y_test)\n",
    "test_masks = torch.tensor(test_attention_masks)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n",
    "\n",
    "test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n",
    "\n",
    "#Modeling\n",
    "cuda = torch.device('cuda')\n",
    "\n",
    "config = BertConfig.from_pretrained('beomi/kcbert-base')\n",
    "config.num_labels = 6\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"beomi/kcbert-base\",\n",
    "                                                         config = config).to(cuda)\n",
    "\n",
    "#Hyperparameter\n",
    "\n",
    "epochs = 10\n",
    "learning_rate = 0.0001\n",
    "optimizer = AdamP(model.parameters(), lr=learning_rate)\n",
    "\n",
    "#Training\n",
    "\n",
    "losses = []\n",
    "accuracies = []\n",
    "epoch_cnt = 1\n",
    "\n",
    "for i in range(epochs):\n",
    "  total_loss = 0.0\n",
    "  correct = 0\n",
    "  total = 0\n",
    "  batches = 0\n",
    "\n",
    "  print(f\"{epoch_cnt} Training...\")\n",
    "\n",
    "  model.train()\n",
    "\n",
    "  for input_ids_batch, attention_masks_batch, y_batch in tqdm(train_dataloader):\n",
    "    optimizer.zero_grad()\n",
    "    y_batch = y_batch.to(cuda)\n",
    "    y_pred = model(input_ids_batch.to(cuda), attention_mask=attention_masks_batch.to(cuda))[0]\n",
    "    loss = F.cross_entropy(y_pred, y_batch)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    total_loss += loss.item()\n",
    "\n",
    "    _, predicted = torch.max(y_pred, 1)\n",
    "    correct += (predicted == y_batch).sum()\n",
    "    total += len(y_batch)\n",
    "\n",
    "    batches += 1\n",
    "    if batches % 500 == 0:\n",
    "      print(\"Batch Loss:\", total_loss, \"Train_accuracy:\", correct.float() / total)\n",
    "  \n",
    "  losses.append(total_loss)\n",
    "  accuracies.append(correct.float() / total)\n",
    "  print(\"Train Loss:\", total_loss, \"Train_accuracy:\", correct.float() / total)\n",
    "\n",
    "  print(\"\")\n",
    "  print(\"Validation...\")\n",
    "\n",
    "  model.eval()\n",
    "\n",
    "  # 변수 초기화\n",
    "  valid_correct = 0\n",
    "  valid_total = 0\n",
    "\n",
    "  for input_ids_batch, attention_masks_batch, y_batch in tqdm(validation_dataloader):\n",
    "    y_batch = y_batch.to(cuda)\n",
    "    y_pred = model(input_ids_batch.to(cuda), attention_mask=attention_masks_batch.to(cuda))[0]\n",
    "    _, predicted = torch.max(y_pred, 1)\n",
    "    valid_correct += (predicted == y_batch).sum()\n",
    "    valid_total += len(y_batch)\n",
    "\n",
    "  epoch_cnt += 1\n",
    "\n",
    "  optimizer = AdamP(model.parameters(), lr=learning_rate*0.8)\n",
    "  \n",
    "  print(\"Validatoion_accuracy:\", valid_correct.float() / valid_total)\n",
    "  print(\"Next Epoch\")\n",
    "  print(\"\")\n",
    "\n",
    "# Model Save\n",
    "torch.save(model.state_dict(), \"/content/drive/MyDrive/비타민 컨퍼런스/Model/kcbert_4\")\n",
    "\n",
    "# Test\n",
    "\n",
    "file_path = \"./Model/kcbert_3\"\n",
    "model.load_state_dict(torch.load(file_path))\n",
    "model.to(cuda)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "\n",
    "for input_ids_batch, attention_masks_batch, y_batch in tqdm(test_dataloader):\n",
    "  y_batch = y_batch.to(cuda)\n",
    "  y_pred = model(input_ids_batch.to(cuda), attention_mask=attention_masks_batch.to(cuda))[0]\n",
    "  _, predicted = torch.max(y_pred, 1)\n",
    "  test_correct += (predicted == y_batch).sum()\n",
    "  test_total += len(y_batch)\n",
    "\n",
    "print(\"Accuracy:\", test_correct.float() / test_total)\n",
    "\n",
    "# New Data Preprocessing \n",
    "\n",
    "# 입력 데이터 변환\n",
    "def convert_input_data(sentences):\n",
    "    global tokenizer\n",
    "    tokenized_texts = tokenizer.tokenize(sentences)\n",
    "    MAX_LEN = 128\n",
    "    input_ids = [[tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]]\n",
    "    input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "    attention_masks = []\n",
    "    \n",
    "    for seq in input_ids:\n",
    "        seq_mask = [float(i>0) for i in seq]\n",
    "        attention_masks.append(seq_mask)\n",
    "\n",
    "    inputs = torch.tensor(input_ids)\n",
    "    masks = torch.tensor(attention_masks)\n",
    "\n",
    "    return inputs, masks\n",
    "\n",
    "def logits_to_softmax(logits):\n",
    "  odds = np.exp(logits)\n",
    "  total = odds.sum()\n",
    "  softmax = odds/total\n",
    "  return softmax\n",
    "\n",
    "def classify_sentence(sentence):\n",
    "  model.eval()\n",
    "\n",
    "  inputs, masks = convert_input_data(new_sentence)\n",
    "  b_input_ids = inputs.to(cuda)\n",
    "  b_input_mask = masks.to(cuda)\n",
    "\n",
    "  with torch.no_grad():     \n",
    "    outputs = model(b_input_ids, \n",
    "                    token_type_ids=None, \n",
    "                    attention_mask=b_input_mask)\n",
    "\n",
    "  logits = outputs[0]\n",
    "  logits = logits.detach().cpu().numpy()[0]\n",
    "\n",
    "  result = logits_to_softmax(logits)\n",
    "\n",
    "  emotion_dict = {0:\"angry\",1:\"disgust\",2:\"fear\",3:\"happy\",4:\"sad\",5:\"surprise\"}\n",
    "\n",
    "  for i in range(len(result)):\n",
    "    print(f\"{emotion_dict[i]} : {round(result[i]*100,3)}%\")\n",
    "\n",
    "new_sentence = \"올해도 좋은일만 가득하길!\"\n",
    "classify_sentence(new_sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
